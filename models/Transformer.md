# Transformer

분류(Labeling)되지 않은 데이터를 사용해 비지도학습을 미리 훈련시킨 이후, 지도학습 과정에서 강화학습(fine-tuned)으로 보정해 성능을 개선하는 구조

## 구성

### Encoder

입력 과정에서, 연속되는 데이터(sequence)의 특성을 추출해 여러 vector로 변환하고 디코더로 전달한다.
> Vector : 연속되는 데이터에서의 위치와 의미 정보를 표현

### Decoder

출력 과정에서, 인코더로부터 받은 정보를 받아 문맥을 유추하고, 출력 데이터를 생성한다.

### 구조 및 동작

트랜스포머(Transformer)는 인코더(Encoder)와 디코더(Decoder)로 구성되며, 각 요소 위에 여려겹의 인코더를 쌓아올린 구조이다.

트랜스포머는 연속된 토큰(token, ~문장에서의 단어)을 가지고 연속된 학습을 진행하고, 출력될 문장에서 다음에 나올만한 단어를 예측한다.

트랜스포머 모델은 attention(또는 self-attention) 매커니즘을 사용해 연속된 데이터에서 멀리 떨어진 데이터에 대해서도 미묘하 관계를 감지한다. 이러한 기술은 각 단어를 개별로 인식하는 대신에, 입력된 연속 데이터에서 항목들 주변의 문맥을 제공하는 방법으로각 단어가 모여 의미하는 문맥을 식별한다.

또한 다중의 입력된 데이터를 병렬처리해서 훈련 단계의 속도를 높일 수 있다.



