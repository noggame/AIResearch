**Attention** : ["Neural Maching Translation by Jointly Learning to Align and Translate"](https://arxiv.org/pdf/1409.0473.pdf)

**Transformer** : ["Attention is all you need"](https://arxiv.org/pdf/1706.03762.pdf)

**GPT**
- GPT4ALL : [Training an Assistant-style Chatbot with Large Scale Data Disillation from GPT-3.5-Turbo](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf)
- GPT-4 : [GPT-4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf)

**BERT** : ["Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805)

**LlaMa** : [LLaMa](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)

**LlaMa2** : [LlaMa2](https://arxiv.org/pdf/2307.09288.pdf)

**Few-Shot Learning** : [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)

LoRA

PEFT

