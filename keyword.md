
RLHF (Reinforcement Learning from Human Feedback) : 강화학습 과정에서 출력된 예측값들에 대해 사람이 어느 것이 더 정답에 가까운지 직접 가중치를 부여하는 방식
